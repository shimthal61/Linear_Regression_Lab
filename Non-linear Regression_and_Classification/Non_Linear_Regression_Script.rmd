---
output: reprex::reprex_document
knit: reprex::reprex_render
---

# Non-Linear Regression

Let's first read in our data for this lab and attach the `Wage` data

```{r}
library(ISLR2)
attach(Wage)
```

## Polynomial Regression

Let's first fit our model using a fourth-degree Polynomial  in `age`

```{r}
poly_fit <- lm(wage ~ poly(age, 4), data = Wage)
coef(summary(poly_fit))
```

The model above returns a matrix whose columns are a basis of orthologonal polynomials, which essentially means that each column is a linear combination of the variables `age`, `age^2`, `age^3`, and `age^4`.

We can instead using the `raw = T` argument to directly obtain each `age` polynomials

```{r}
poly2_fit <- lm(wage ~ poly(age, 4, raw = TRUE), data = Wage)
coef(summary(poly2_fit))
```

We can create a similar model using the `cbind()` function for building a matric from a collection of vectors.

Let's now create a grid of values for `age` at which we want predictions, and then call then generic `predict()` function, specifying that we want standard error as well

```{r}
agelims <- range(age) # Create a vector with the min and max ages
age_grid <- seq(from = agelims[1], to = agelims[2]) # Creates a vector which contains all the values from the first index of age to the last index
preds <- predict(poly_fit, newdata = list(age = age_grid),
    se = TRUE)
se_bands <- cbind(preds$fit + 2 * preds$se.fit,
    preds$fit - 2 * preds$se.fit)
```

Finally, we plot the data and add the fit from the degree-4 polynomial.

```{r}
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1),
    oma = c(0, 0, 4, 0)) # Set the physical margins and parameters of the plot
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey") # Create the plot using the vectors we created earlier
title("Degree-4 Polynomial", outer = TRUE) # Set the title and location
lines(age_grid, preds$fit, lwd = 2, col = "blue") # Create the fit line from the 4th degree polynomial
matlines(age_grid, se_bands, lwd = 1, col = "blue", lty = 3) # Add in the standard error line
```

We can also see whether using an orthogonal set of basic functions will affect our model in a meaningful way

```{r}
pred2 <- predict(poly2_fit, newdata = list(age = age_grid),
    se = TRUE)
max(abs(preds$fit - pred2$fit))
```

## What degree Polynomial

When performing polynomial Regression we must decide on the degree of the polynomial to use. We can use a null-hypothesis test to determine the simpliest model to explain the relationship between `wage` and `age`.

```{r}
for (i in 1:5) { # We first create a for loop with i having a value 1 to 5
    nam <- paste("fit_", i, sep = "") # We create a new variable called fit_i
    assign(nam, lm(wage ~ poly(age, i), data = Wage)) # We carry out a polynomial fit of i and assign it to fit_i
}
anova(fit_1, fit_2, fit_3, fit_4, fit_5) # We run an anova to test the null hypothesis]
```

The *p*-values comparing the linear model to the quadratic model is essentially zero, indicating that the linear fit it not sufficient. 

Continuing this logic, it appears as though a cubic or quadratic polynomial appears to provide the most reasonable fit to the data. 

Instead of using the `anova()` function, we could have obtained these p-values more succinctly by exploting the fact that `poly()` creates orthogonal polynomials.

```{r}
coef(summary(fit_5))
```

Notice how the *p*-values are the same, with the *t*-statistic equal to the F-statistic from the `anova()`. 

The ANOVA method also works when we have more terms in our model

```{r}
for (i in 1:5) {
    nam <- paste("fit_", i, sep = "")
    assign(nam, lm(wage ~ education + poly(age, i), data = Wage))
}
anova(fit_1, fit_2, fit_3, fit_4, fit_5)
```

# Step functions

In order to fit a step function, we use the `cut()` function. This returns an ordered categorical variable.

```{r}
table(cut(age, 4))
```

```{r}
step_fit <- lm(wage ~ cut(age, 4), data = Wage)
coef(summary(step_fit))
```

Here, `cut()` automatically picked the cutpoints at 33.5, 49, and 64.5 years of age. 

We can also specify our own cutpoints using the `breaks` argument. 


We can create another visualisation with using the same code as before, except we pass through our step function model instead

## Creating a plot

```{r}
preds <- predict(step_fit, newdata = list(age = age_grid),
    se = TRUE)
se_bands <- cbind(preds$fit + 2 * preds$se.fit,
    preds$fit - 2 * preds$se.fit)
```

```{r}
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1),
    oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Degree-4 Step Function", outer = TRUE)
lines(age_grid, preds$fit, lwd = 2, col = "blue")
matlines(age_grid, se_bands, lwd = 1, col = "blue", lty = 3)
```

## Deciding which degree

Like with the polynomial regression, we can create a list of different cuts at age within a `for` loop

```{r}
for (i in 2:8) {
    nam <- paste("step_", i, sep = "")
    assign(nam, lm(wage ~ cut(age, i), data = Wage))
}
anova(step_2, step_3, step_4, step_5, step_6, step_7, step_8)
```

```{r}
coef(summary(step_8))
```

# Splines

In order to fit regression splines, we use the splines library. 

```{r}
library(splines)
```

Regression splines can be fit by constructing an appropriate matrix of basic functions. 

The `bs()` function (basis splines) generates the entire matrix of basis functions for splines with the specified set of knots. 

By default, cubic splines are produced. 

```{r}
par(new)
spl_fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage) # Here we specify the knots at 25, 40, and 60.
pred <- predict(spl_fit, newdata = list(age = age_grid), se = TRUE)
plot(age, wage, col = "grey")
lines(age_grid, pred$fit, lwd = 2)
lines(age_grid, pred$fit + 2 * pred$se, lty = "dashed")
lines(age_grid, pred$fit - 2 * pred$se, lty = "dashed")
```

We can use the `dim()` function to retrieve the dimensions of an object

```{r}
dim(bs(age, knots = c(25, 40, 60)))
```

We can also use the `attr()` function to get specific attributes of an object

```{r}
attr(bs(age, df = 6), "knots")
```

In this case  R  chooses knots at ages 33,8, 42.0 and 51.0, which correspond to the 25th, 50th, and 75th percentiles of `age`. The function `bs()` 
also has a  degree  argument, so we can fit splines of any degree, rather than the default degree of 3 (which yields a cubic spline).

## Fitting a natural spline

In order to fit a natural spline, we use the `ns()` (natural spline) function.

```{r}
nat_fit <- lm(wage ~ ns(age, df = 4), data = Wage) # Here, we fit a natural spline with 4 degrees of freedom
pred2 <- predict(nat_fit, newdata = list(age = age_grid),
            se = TRUE)
par(new = TRUE)
plot(age, wage, col = "gray")
lines(age_grid, pred2$fit, col = "red", lwd = 2)
```

As with the `bs()` function, we can instead manually specify the knots using the `knots` argument

```{r}
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1),
    oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Smoothing Splines")
fit <- smooth.spline(age, wage, df = 16) # In our first model, we specify df = 16.
fit2 <- smooth.spline(age, wage, cv = TRUE) # We then select the smoothness level using cross-validation
fit2$df # Using cross-validation has resulted in a df value of 6.8
lines(fit, col = "red", lwd = 2)
lines(fit2, col = "blue", lwd = 2)
legend("topright", legend = c("16 DF", "6.8 DF"),
     col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
```

# GAMs

We can fit a GAM to predict `wage` using natural spline functions of `lyear` and `age`, treating `education` as a qualitative predictor.

Since this is effectively just a big linear regression model, we can use the `lm()` function

```{r}
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education,
    data = Wage)
```

We need to use the `gam()` library 

```{r}
library(gam)
```

```{r}
gam_m3 <- gam(wage ~
    s(year, 4) + # The s() function indicates the use of smoothing splines, and we specify 4 df
    s(age, 5) + # We do the same here except we want 5 df
    education, # Education is qual, and is converted into four dummy variables
    data = Wage) # The s() function indicates the use of smoothing splines
```

```{r}
par(mfrow = c(1, 3))
plot(gam_m3, se = TRUE, col = "blue")
```

```{r}
plot.Gam(gam1, se = TRUE, col = "red") # We have to use plot.Gam() instead
```

In the first plot, `lyear` looks rather lienar. We can perform AVOVA tests to determine which model is best:
- One that excludes `lyear`
- One what uses a linear function of `lyear`
- GAM that uses a spline function of `lyear`

```{r}
gam_m1 <- gam(wage ~ s(age, 5) + education, data = Wage)
gam_m2 <- gam(wage ~ year + s(age, 5) + education, data = Wage)
anova(gam_m1, gam_m2, gam_m3)
```

Here, we find compelling evidence that a GAM with a lienar function of `lyear` is better than one without.

However, there is no evidence that a non-linear function of `lyear` is needed. 

The `summary()` function produces a summary of the GAM fit

```{r}
summary(gam_m3)
```

The `Anova for Parametric Effects` *p*-values clearly demonstrate that `year`, `age` , and `education` are all highly statistically significant, evenwhen only assuming a linear relationship. 
Alternatively, the `Anova for Nonparametric Effects` *p*-values for `year` and `age` correspond to anull hypothesis of a linear relationship versus the alternative of a non-linear relationship. 
The large *p*-value for  year  reinforces our conclusion from the ANOVA test that a linear function is adequate for this term. 
However, there is very clear evidence that a non-linear term is required for `age`. We can make predictions using the `predict()` method for the class `Gam`. 
Here we make predictions on the training set.

```{r}
preds <- predict(gam_m2, newdata = Wage)
```