---
title: "Classification"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float: yes
    font:family: Lato
  pdf_document:
    toc: yes
---

First, we load in and attach our dataset

```{r}
library(ISLR2)
names(Smarket)
```

```{r}
dim(Smarket) # View the number of rows and columns
```

We can use the `pairs()` function to create a scatterplot matrix of all the variables.

The `cor()` function produces a matrix of all the pairwise correlations among the predictors.
We have to omit `direction` as it is a qualitative predictor

```{r}
pairs(Smarket)
cor(Smarket[, -9])
```

We can see that the correlations between `lag` variables and `Today` returns are close to zero,
In other words, there appears to be little correlation between today's returns and the previous days' returns.
The only substantial correlation is between `year` and `volume`.

By plotting the data, we see that `volume` is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005

```{r}
attach(Smarket)
plot(Volume)
```

# Logisitic Regression

Next, we will fit a logisitic regression model to predict `direction` using `lagone`.

The `glm()` can be used to fit many types of generalised linear models. 
The syntax is similar to `lm()` except that we must specify that the family is binomial.

```{r}
log_fit <- glm(Direction ~ . - Year - Direction - Today,
               data = Smarket, family = binomial)
summary(log_fit)
```

The smallest *p*- value here is `Lag1`. The negative coefficient suggests that if the market had a positive return yesterday, then is it less likely to go up today.
However, the *p*-value is still not significant, so there is no clear evidence for a real assocaition between `lag1` and `direction`.

We can use the `coef()` function to retrieve the coefficients for this model.

```{r}
coef(log_fit)
```

Alternatively we can access it using `$`.

```{r}
summary(log_fit)$coef[, 4]
```

The `predict()` function can be used to predict the probability that the market will go up, using our predictors. 

```{r}
log_prob <- predict(log_fit, type = "response")
log_prob[1:10]
```

We know that the above values correspond to the probability of the market going up, since the `contrasts` function tells us how the levels have been dummy coded

```{r}
contrasts(Direction)
```

In order to make a prediction as to whether the market will go up or down, we must convert these predicted probabilites to class labels: `Up` or `Down`. 
The following commands create a vector of class predictions based on the predicted probability of a market increase is greater or less than 0.5

```{r}
log_pred <- rep("Down", 1250)
log_pred[log_prob > .5] <- "Up"
```

The first command creates a vector of 1250 `Down` elements. The second line transforms to `Up` all of the elements which the predicted proability of a market increase exceeds 0.5

Given these predictions, the `table()` function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classificed

By inputting two qualitative vectors, `R` will create a 2x2 table with counts of the number of times each combincation occured.

```{r}
confusion <- table(log_pred, Direction)
confusion <- as.data.frame(confusion) # Converted the table to a df
sum((confusion$Freq[1] + confusion$Freq[4]) / sum(confusion$Freq)) # Work out the sum of correctly predicted
```

In this case, the Logisitic regression correctly predicted the movement of the market 52.2% of the times.

At first glance, it appears as though our logistic regression model is working a little better than chance. 
However, this is misleading since we trained and tested the model on the same set of observations. 
The training error rate is often overly optimistic - it tends to underestimate the test error rate.
In order to better assess the accuracy of the model, we can fut the model using part of the data, and then examine how well it predicts the held out data.
This will yield a more realistic error rate.

To impliment this strategy, we first create a vector corresponding to the observation from 2001 though 2004. We will then use this vector to create a held out data set of observations from 2005

```{r}
train <- (Year < 2005)
smarket_05 <- Smarket[!train, ]
dim(smarket_05)
```

```{r}
direction_05 <- Direction[!train]
```

The object `train` is a vector containing all the observations, with boolean variables corresponding to the years. All observations before 2005 are `TRUE`, and all in 2005 are `FALSE`. The `!` symbol reverses all the boolean elements. We now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005. We then obtain predicted probabilities of the stock market going up for each day in our test data

```{r}
log_fit <- glm(Direction ~ . - Year - Today - Direction,
               data = Smarket, family = binomial, subset = train)
```

Now that we've trained our data, we can test it on a our test data set

```{r}
log_probs <- predict(log_fit, smarket_05, type = "response")
```

Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period

```{r}
log_pred <- rep("Down", 252)
log_pred[log_probs > .5] <- "Up"
table(log_pred, direction_05)
```

```{r}
mean(log_pred == direction_05) # Computes the error rate
mean(log_pred != direction_05) # Computes the test set error rate
```

The test error rate is 51.98%, worse than guessing at chance! We recall that the original logistic regression model had pretty large *p*-values associated with the predictor variables. Perhaps by removing the variables that appear not to be helpful in predicting `direction`, we can obtain a more effective model, since these variables might be adding to the error rate. 

```{r}
summary(log_fit) # Have another look at the p-values
```

Below, we refit the logistic regression using just `lagone` and `lagtwo`, which seemed to have the largest predictive power in the original logistic regression model. 

```{r}
log_fit <- glm(Direction ~ Lag1 + Lag2, data = Smarket,
                family = binomial, subset = train)
log_probs <- predict(log_fit, smarket_05, type = "response")
log_pred <- rep("Down", 252)
log_pred[log_probs > .5] <- "Up"
table(log_pred, direction_05)
mean(log_pred == direction_05)
```

This is a little better - 56% of the daily movements have now been correctly predicted. Suppose that we want to predict the returns associated with particular values of `lagone` and `lagtwo`. In particular, we want to predict `direction` on a day when `lagone` and lagtwo` equal 1.2 and 1.1 respectively, and we a day when they equal 1.5 and 0.8. We do this using the `predict()` function

```{r}
predict(log_fit, newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
            type = "response")
```

# Linear Disciminant Analysis

We will now perform LDA on the `Smarket` data. We can fit an LDA mode using the `lda()` function, which is part of the `MASS` library. The syntax is identical, to `lm`, except we omit the `family` argument. We'll fit the model using our training data created before.

```{r}
library(MASS)
lda_fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
lda_fit
```

```{r}
lda_fit$prior
```

The output from `lda_fit` indicates that the prior probability the market went down was 49.2%, and the probability that it went up was 50.8%.

```{r}
lda_fit$means
```

It also provides the group means - these are the averages of each predictor within each class. These suggests that there is a tendency for the previous 2 day's returns to be negative on days when the market increases, and a tendency for the previous days' returns to be positive on days when the market declines.

```{r}
lda_fit$scaling
```

The coefficients provides the linear combination of `lagone` and `lagtwo` that are used to form the LDA decision rule. 

The `plot()` function produces plots of the linear discriminants. The `Up` and `Down` observations are displayed separately. 

```{r}
plot(lda_fit)
```

The `predict()` function returns a list with three elements:

- The first element, `class`, contains LDA's predictions about the movement of the market.
- The second element, `posterior` is a matrix whose *k*th column contains the posterier probability that the corresponding observation belongs to the *k*th class.
- Finally `x` contains the linear discriminants, described earlier.

```{r}
lda_pred <- predict(lda_fit, smarket_05)
names(lda_pred)
```

The `LDA` and `logistic regression` predictions are almost identical:

```{r}
lda_class <- lda_pred$class
table(log_pred, direction_05) # Logistic regression table
table(lda_class, direction_05) # Linear Discriminant table
```

```{r}
mean(lda_class == direction_05)
```

Applying a 50% threshold to the posterier probabilities allows us to recreate the predictions contained in `lda_class`

```{r}
sum(lda_pred$posterior[, 1] >= .5)
sum(lda_pred$posterior[, 1] < .5)
```

Notice that the posterior probability output by the model corresponds to the proability that the market will decrease.

```{r}
table(lda_class, direction_05)
```

If we wanted to use a posterior probability threshold other than 50% in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market increase only if we are very certain that the market will indeed increase on that day, say, if the posterior probability is at least 90%/

```{r}
sum(lda_pred$posterior[, 1] > .9)
```

This tells us that no days in 2005 meet the threshold. In fact, the greatest posterior probability of decreases in all of 2005 was 52.02%.

# Quadratic Discriminant Analysis

We will not fit a QDA model to the `Smarket` data. QDA is implimented using the `qda()` function, also part of the `MASS` library. The syntax is equal to that of `lda()`. 

```{r}
qda_fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
qda_fit
```

This output contains the group means, but it does not contain the coefficients for the linear discriminants. This is because the QDA classifier is a quadratic, rather than a linear, function of the predictors. The `predict()` function works in exactly the same fashion as for LDA.

```{r}
qda_class <- predict(qda_fit, smarket_05)$class
table(qda_class, direction_05)
```

We can assess the model accuracy simply by using the `mean()` function.

```{r}
mean(qda_class == direction_05)
```

Interestingly, the QDA predictors are accurate almost 60% of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, it is recommended to evaluate this method's performance on a larger test set before assuming that this approach will consistently perform better.

# Naive Bayes

Next, we fit a naive Bayes model to the `Smarket` data. We can use the `naiveBayes()` function, which is part of the `e1071` library. The syntax is identical to the previous models. By default, each quantitive feature is modelled using a Gaussian distribution. However, a kernel densitiy method can also be used to estimate distributions,

```{r}
library(e1071)
nb_fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
nb_fit
```

The output estimates the mean and SD for each variable in each class. For example, the mean for `lagone`  when `Direction==Down` is 0.0428, and the SD is 1.23. 

```{r}
nb_fit$table$Lag1[1]
```

The `predict()` function is straightforward

```{r}
nb_class <- predict(nb_fit, smarket_05)
table(nb_class, direction_05)
```

```{r}
mean(nb_class == direction_05)
```

Naive Bayes works very well on this dataset, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA. 

The `predict()` function can also generate estimates of the probability that each observation belongs to a particular class.

```{r}
nb_preds <- predict(nb_fit, smarket_05, type = "raw")
nb_preds[1:5, ]
```

# K-Nearest Neighbors

We can perform KNN using the `knn()` function, which is part of the `class` library. This function works differently to other model-fitting functions. Rather than a two-step approach where we first fit the model and then use the model to make predictions, `knn()` makes predictions using a single command. The function requires four inputs:

- A matrix containing the predictors associated with the training data, labelled `train_X`.
- A matrix containing the predictors associated with the data for which we wish to make predictions, labelled `test_X`.
- A vector containing the class labels for the training observations, labelled `train_direction`.
- A value for *`K`*, the number of nearest neighbors to be used for the classifier.

We use the `cbind()` function, short for *column bind*, to bind the `lagone` and `lagtwo` variables together into the two matrices, one for the training set and the other for the test set.

```{r}
library(class)
train_x <- cbind(Lag1, Lag2)[train, ]
test_x <- cbind(Lag1, Lag2)[!train, ]
train_direction <- Direction[train]
```

Now the `knn()` function can be used to predict the markest's movements for the dates in 2005. We set a random seed using `set.seed()`, because if several observations are tied as nearest neighbors, then `R` will randomly break the tie. Therefore, a seed must be sed in order to ensure reproducibility. 

```{r}
set.seed(42)
knn_pred <- knn(train_x, test_x, train_direction, k = 1)
table(knn_pred, direction_05)
```

We can find the success rate easily by calculating the percentage of correctly identified classifier

```{r}
(83 + 43) / 252
```

The results of using *`K`* = 1 are not very good, since only 50% of the observations are correctly predicted. Of course, it may be that *`K`* = 1 is an overly flexible fit to the data. Below, we repeat the analysis using *`K`* = 3.

```{r}
knn_pred <- knn(train_x, test_x, train_direction, k = 3)
table(knn_pred, direction_05)
mean(knn_pred == direction_05)
```

The results are slightly better, however increasing *`K`* further provides no further improvements. For our dataset, is appears as though QDA provides the best results out of the methods we have examined so far.

## KNN on `Insurance` Dataset

This data set includes 85 predictors that measure demographic characteristics for 5822 individuals. The output variable is `Purchase`, which indicates whether or not a given individual purchanes a caravan insurance policy. In this data set, only 6% of people purchaes caravan insurance

```{r}
dim(Caravan)
```

```{r}
attach(Caravan)
summary(Purchase)
```

KNN predicts the class of a given test observation by identifying the observations that are nearest to it, meaning the scale of variables matters. Variables on a large scale will have a much larger effect on the *distance* between the observations, and hence on the KNN classifier, than variables on a small scale. For instance, imagine a dataset that contains two variables, `salary` and `age`. When using KNN, a difference of $1000 in salary is huge compared to a difference of 50 years in age. Consequently, `salary` will drive the KNN classifier results, and `age` will have near enough no effect. This is contrary to our intuition that a salary difference of $1000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured `salary` in Japanese yen, or if we measured age in minutes, then we'd get a quite difference classification result if these variables were measured using a different scale.

A good way to handle this problem is standatise the data so that all the variables are given a mean of 0 and an SD of one. Then, all variables will be a compariable scale. The `scale()` function does just this. By standatising the data, we exlude column 86, because it is a qualititive predictor, `Purchase`.

```{r}
standardised_x <- scale(Caravan[, -86])
```

Every column of `standardised_x` has an SD of 1 and a mean of 0.

We now split the observations into a test set, containing the first 1000 observations, and a training set, containing the remaining observations.

```{r}
# Creates a vector from values 1 - 1000
test <- 1:1000

# Creates a submatrix containing the observations not ranging 1 - 1000
train_x <- standardised_x[-test, ]

# Creates a submatrix containing the observations ranging 1 - 1000
test_x <- standardised_x[test, ]
train_y <- Purchase[-test]
test_y <- Purchase[test]
```

We then fit a KNN model on the training data using *`K`* = 1, and evaluate its performance on the test data.

```{r}
set.seed(42)
knn_pred <- knn(train_x, test_x, train_y, k = 1)
mean(test_y != knn_pred)
```

```{r}
mean(test_y != "No")
```

The KNN error rate on the 100 test observations is just over 11%. Although this may appear to be fairly good, only 6% of customers purchased insurance. We could get the error rate down to 6% by always predicting `No` regarless of the values of the predictors!

Suppose there is some cost to trying to sell insurance to a given individual. If the company tries to sell insurance to a random selection of customers, then the success rate will only be 6%, which may be far too low given the costs involved. Instead, the company would like to sell insurance only to customers who are likely to buy it. So the overall error rate is not of interest, instead, the fraction of individuals that are correctly predicted to buy insurance is of interest. 

It turns out that KNN with *`K`* = 1 does far better than random guessing among the customers that are predicted to buy insurance. Among 77 such customers, 9, or 11.3%, actually do purchase insurance. This is double the rate that one would obtain from random guessing. 

```{r}
table(knn_pred, test_y)
```

Using *`K`* = 3, the success rate increases to 19%, and with *`K`* = 5, the rate is 26.7%. This is over four times the rate that results from random guessing. It appears that KNN is finding some real patterns in a difficult data set!

```{r}
knn_pred <- knn(train_x, test_x, train_y, k = 3)
table(knn_pred, test_y)
5 / 26
```

```{r}
knn_pred <- knn(train_x, test_x, train_y, k = 5)
table(knn_pred, test_y)
4 / 15
```

However, it is still worth noting that only 15 customers are predicted to purchase insurance using KNN with *`K`* = 5. In practice, the insurance company will wish to expend resources on convincing more than just 15 potential customers to buy insurance. 

As a comparison, we can also fit a logistic regression model to the data. If we use 0.5 as the predicted probability cut-off for the classifier, then we have a problem: only seven of the test observations are predicted to purchase insurance. Even worse, we are wrong about all of these! However, we are not required to use a cut-off of 0.5.

```{r}
glm_fit <- glm(Purchase ~ ., data = Caravan, family = binomial, subset = -test)
glm_prob <- predict(glm_fit, Caravan[test, ], type = "response")
glm_pred <- rep("No", 1000)
glm_pred[glm_prob > .5] <- "Yes"
table(glm_pred, test_y)
```

If we instead predict a purchase any time the predicted probability of purchase exceeds 0.25, we get much better results: we predict that 33 people will purchase insurance, and we are correct for about 33% of these people. That is over five times better than random guessing!

```{r}
glm_pred[glm_prob > .25] <- "Yes"
table(glm_pred, test_y)
11 / (22 + 11)
```