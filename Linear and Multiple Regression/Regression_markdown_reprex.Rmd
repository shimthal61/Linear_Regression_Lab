```{r reprex-options, include = FALSE}
options(
  keep.source = TRUE,
  rlang_backtrace_on_error_report = "full",
  crayon.enabled = FALSE,
  reprex.current_venue = "gh"
)
```

```{r, results = 'asis', echo = FALSE, include = file.exists('.Rprofile'), eval = file.exists('.Rprofile')}
cat(sprintf("*Local `.Rprofile` detected at `%s`*", normalizePath(".Rprofile")))
```

---
output: reprex::reprex_document
knit: reprex::reprex_render
---

# Libraries

Load in our packages

```{r, message=FALSE}
library(MASS) # Contains our data
library(ISLR2) # Contains the Boston data set
```

Let's glimpse our dataset

```{r}
head(Boston)
```

Output variable:
- medv - medium house value. Our predictor variable

Input variables:
- rmvar - average number of rooms per house
- age average age of house 
- lstat - percent of households with low socioeconomic status

# Simple Linear Regression

Let's use the `attach` command to lock on our data set

```{r}
attach(Boston)
```

We can use the `lm` function to fit a simple linear regression model

```{r}
lm_fit <- lm(medv ~ lstat)
```

We can view some of the characteristics of the model
```{r}
lm_fit
```

For more detail information, we use `summary` function to give us the *p*-values and standard errors for the coefficients and the R^2 statistic and *F*-statistics

```{r}
summary(lm_fit)
```

We can use the `names()` function to find out more information in our model. 

```{r}
names(lm_fit)
```

We can also use the `coef()` function to find out the coefficients

```{r}
coef(lm_fit)
```

The `confint()` function shows us confidence intervals

```{r}
confint(lm_fit)
```

The `predict()` function shows us CI and prediction intervals for the prediction of `medv` for a given value of `lstat`

```{r}
predict(lm_fit, data.frame(lstat = (c(5, 10, 50))),
    interval = "confidence")
```

```{r}
predict(lm_fit, data.frame(lstat = (c(5, 10, 15))),
    interval = "prediction")
```

For example, the 95% CI associated with an `lstat` value of 10 is (24.47, 25. 63), and the 95% prediction interval is (12.83, 37.28).

## Creating our plots

We plot `medv` and `lstat` along with the least squares regression line using `plot` and `abline`
functions
```{r}
plot(lstat, medv)
abline(lm_fit)
```

By looking at the data, there is some evidence for non-linearity -  we will explore this later.

The `abline()` function can be used to draw any line, not just the least squares regression line. 

```{r}
plot(lstat, medv, pch = "x")
abline(lm_fit, lwd = 3, col = "red")
```

## Diagnostic plots

We can create some Diagnostic plots using the `plot()` function to `lm_fit`. We can use the `par()` function and `mrow()` argument to tell R to split the plots into separate panels so we can view multiple plots are once. 

```{r}
par(mfrow = c(2, 2))
plot(lm_fit)
```

On the basis of the residuals vs fitted plot, there is some evidence of non-linearity. Leverage statisticcan be computer for any number of predictors using the `hatvalues()` function
```{r}
par(mfrow = c(1, 1))
plot(hatvalues(lm_fit))
```

The `which.max()` function identifies the index of the largest element of a vector. In this case it tells us which observation has the largest leverage statistic.

```{r}
which.max(hatvalues(lm_fit))
```

# Multiple Regression

To fit a multiple linear regression model using least squares, we again need to use the `lm()` function. 

```{r}
mult_model <- lm(medv ~ lstat + age)
summary(mult_model)
```

If we want to add our of our predictor, we can instead use `.,`

```{r}
mult_model <- lm(medv ~ ., data = Boston)
summary(mult_model)
```

We can also remove variables using `-`

```{r}
mult_model <- lm(medv ~ . - age, data = Boston)
```

Alternatively, the `update` function can be used

```{r}
mult_model <- update(mult_model, ~ . - age)
```


We can access individual features of a summary object by name using `$`

```{r}
summary(mult_model)$r.sq
```

The `vif()` function from the `car` package can be used to compute variance inflation factors.

```{r}
library(car)
vif(mult_model)
```

# Interaction Terms

We can add interaction terms within `lm()` using `*`. 

```{r}
summary(lm(medv ~ lstat * age, data = Boston))
```

# Non-linear Transformations of the predictors

The `lm()` function can also accomodate non-linear transformations of the predictors. For a given predictor *X*, we can create a predictor *X*^2 using `I(X^2)`. The function `I()` is needed since the `^` has a special meaning.

```{r}
lm_fit_non_lin <- lm(medv ~ lstat + I(lstat^2))
summary(lm_fit_non_lin)
```

The near-zero *p* value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to assess which fit is superior

```{r}
lm_fit <- lm(medv ~ lstat)
anova(lm_fit, lm_fit_non_lin)
```

The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that both models fit the data equally well, and the alternate hypothesis is that the full model is superior. The associated *p*-value is nearly zero. This provides clear evidence that the model containing the predictors `lstat` and `lstat^2 is far superior. This is not surprising, since we saw evidence for non-linearity in the relationship between `medv` and `lstat`.

```{r}
par(mfrow = c(2, 2))
plot(lm_fit_non_lin)
```

Our residuals look much better, there is little discernible pattern in the residuals.

## Cubic fit

In order to create a cubic fit, we can include a predictor or the form `I(X^2)`. However, this approach can start to get cumbersome for higher order polynomials. A better approach involves using the `poly()` function to create the polynomial with `lm()`. 

```{r}
poly_5 <- lm(medv ~ poly(lstat, 5))
summary(poly_5)
```

This suggests that including additional polynomial terms leads to an improvement in the model fit. However, further investigation of the data reveals that no polynomial terms beyond the fifth order have significant *p*-values in a regression fit.

We can also try a log transformation

```{r}
summary(lm(medv ~ log(rm), data = Boston))
```

# Qualitative predictors

Let's have a look at the `Carseats` data:

```{r}
head(Carseats)
```

The qualitative predictor `shelveloc` indicates the shelving location, either bad, medium, and good. R generates dummy variables automatically

```{r}
qual_fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(qual_fit)
```

The `contrasts()` function returns the coding the `R` uses the for dummy variables

```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

<sup>Created on `r Sys.Date()` with [reprex v`r utils::packageVersion("reprex")`](https://reprex.tidyverse.org)</sup>
